from flask import Flask,request,jsonify
import tensorflow as tf
import pickle
import re
from fuzzywuzzy import fuzz
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder

model = tf.keras.models.load_model('model.h5')

with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

with open('label_encoder.pickle', 'rb') as a:
    label = pickle.load(a)

stopwords = []
with open('stopwords.txt', encoding='utf-8') as f:
    for line in f:
        stopwords.append(line.strip())

# fungsi untuk menghilangkan stopwords
def remove_stopwords(text):
    text = text.lower()
    text = re.sub('[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'[.,â€™"\'-?:!;]', '', text)
    text = re.sub(r'^Apa|^Saya|^Aku|^maukah|^Siapa|^Bagaimana|^Dimana|^Kapan|^Kenapa|^Bisakah', '', text)
    words = [word for word in text.split() if word not in stopwords]
    return ' '.join(words)

# fungsi untuk mengganti kata typo dengan kata yang paling mirip pada dataset
def correct_typo(word):
    max_score = -1
    correct_word = word
    for vocab_word in tokenizer.word_index:
        score = fuzz.ratio(word, vocab_word)
        if score > max_score:
            max_score = score
            correct_word = vocab_word
    return correct_word

# fungsi untuk mengoreksi typo pada kalimat
def correct_sentence(sentence):
    corrected_sentence = []
    words = sentence.split()
    for word in words:
        corrected_word = correct_typo(word)
        corrected_sentence.append(corrected_word)
    return ' '.join(corrected_sentence)

# fungsi untuk membersihkan dan memproses input pengguna
def preprocess_input(text):
    text = remove_stopwords(text)
    text = correct_sentence(text)
    return text


#REQUEST
app = Flask(__name__)

@app.route('/')
def index():
    return "NLP Model API Connected Successfully by TEMARA"

@app.route('/bot',methods=['POST'])
def chatbot():
    user_input = request.form.get('user')
    preprocessed_text = preprocess_input(user_input)
    input_seq = tokenizer.texts_to_sequences([preprocessed_text])
    input_seq = pad_sequences(input_seq, maxlen=100)
    predicted_label_seq = model.predict(input_seq)
    predicted_label = label.inverse_transform(predicted_label_seq.argmax(axis=-1))[0]
    return jsonify({'Bot':predicted_label})

@app.route('/status',methods=['GET'])
def status():
    return jsonify({"success":"Server is ready!!"})

if __name__ == '__main__':
    app.run(debug=True, host="0.0.0.0",port=8080)

